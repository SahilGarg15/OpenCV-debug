{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# \u26a1 Lab 2.3: Model Architecture Comparison\n**Module 3: Computer Vision and Image Processing**\nB-Tech AI Specialization | Chitkara University | February 2026\n\n---\n\n## \ud83c\udf53 Industry Scenario\n> A client wants to deploy a classifier on a **Raspberry Pi** \u2014 limited CPU, no GPU. You need to recommend which model architecture to use. The choice requires balancing accuracy with inference speed and memory. You need **real benchmark data** to make the recommendation.\n\n## \ud83c\udfaf Objective\nBenchmark **VGG16**, **ResNet50**, and **MobileNetV2** on inference speed and model size. Recommend which to use for edge deployment.\n\n**Time:** 60 minutes | **Mode:** Individual\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \u2699\ufe0f Setup \u2014 Run First"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from google.colab import output\noutput.enable_custom_widget_manager()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, Code\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n\nprint(f\"TensorFlow: {tf.__version__}\")\nprint(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\nprint(\"\u2705 Ready\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def reveal_button(hint_text, solution_code):\n    import ipywidgets as widgets\n    from IPython.display import display, HTML, Code\n    out = widgets.Output()\n    hint_btn = widgets.Button(description='\ud83d\udca1 Hint', button_style='info',\n        layout=widgets.Layout(width='120px', margin='4px'))\n    sol_btn  = widgets.Button(description='\u2705 Solution', button_style='warning',\n        layout=widgets.Layout(width='140px', margin='4px'))\n    hide_btn = widgets.Button(description='\ud83d\ude48 Hide', button_style='',\n        layout=widgets.Layout(width='100px', margin='4px'))\n    def on_hint(b):\n        with out:\n            out.clear_output(wait=True)\n            display(HTML(f'<div style=\"background:#e3f2fd;padding:12px;border-radius:6px;'\n                f'border-left:4px solid #1976D2;font-size:14px\"><b>\ud83d\udca1 Hint:</b><br>{hint_text}</div>'))\n    def on_sol(b):\n        with out:\n            out.clear_output(wait=True)\n            display(HTML('<b>\u2705 Solution:</b>'))\n            display(Code(solution_code, language='python'))\n    def on_hide(b):\n        with out: out.clear_output()\n    hint_btn.on_click(on_hint); sol_btn.on_click(on_sol); hide_btn.on_click(on_hide)\n    display(widgets.HBox([hint_btn, sol_btn, hide_btn]), out)\n\nprint(\"reveal_button() ready \u2705\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \ud83e\udd14 Predict Before You Benchmark\n\nBefore writing any code, fill in your guesses in the table below. Commit to a number \u2014 we'll compare against real results.\n\n| Model | Your guess: # Parameters | Your guess: Inference time (ms) | Best for? |\n|---|---|---|---|\n| VGG16 | ? M | ? ms | ? |\n| ResNet50 | ? M | ? ms | ? |\n| MobileNetV2 | ? M | ? ms | ? |\n\nAlso answer:\n1. What is a \"residual connection\" (ResNet's key innovation)?\n2. What makes MobileNet \"mobile\" \u2014 what did the designers sacrifice?\n3. On a Raspberry Pi with no GPU, does model size or architecture matter more?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# \u270f\ufe0f Your predictions:\n# VGG16:       params ~_M,  latency ~_ms\n# ResNet50:    params ~_M,  latency ~_ms\n# MobileNetV2: params ~_M,  latency ~_ms\n\n# 1. Residual connections are...\n# 2. MobileNet is \"mobile\" because...\n# 3. On CPU, what matters most is..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Task 2: Load All 3 Models\n\nLoad each model with ImageNet weights. We use `include_top=True` so we get the full model including the classifier head \u2014 this gives us realistic parameter counts.\n\n> \u26a0\ufe0f This will download ~700 MB total. It takes 2\u20133 minutes on Colab \u2014 that's expected."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# TODO: Load all 3 models with imagenet weights\n# VGG16 expects 224x224, ResNet50 expects 224x224, MobileNetV2 expects 224x224\n\nvgg16       = None  # VGG16(weights=..., include_top=...)\nresnet50    = None  # ResNet50(...)\nmobilenetv2 = None  # MobileNetV2(...)\n\nmodels_dict = {\n    'VGG16':       vgg16,\n    'ResNet50':    resnet50,\n    'MobileNetV2': mobilenetv2,\n}\n\n# Quick sanity check \u2014 print total params for each\nfor name, model in models_dict.items():\n    if model is not None:\n        print(f\"{name:<15}: {model.count_params():>12,} parameters\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "reveal_button(\n    hint_text=\"All three use the same pattern: <code>ModelName(weights='imagenet', include_top=True)</code>. \"\n              \"MobileNetV2 input defaults to 224x224 so no need to specify input_shape.\",\n    solution_code=(\n        \"vgg16       = VGG16(weights='imagenet', include_top=True)\\n\"\n        \"resnet50    = ResNet50(weights='imagenet', include_top=True)\\n\"\n        \"mobilenetv2 = MobileNetV2(weights='imagenet', include_top=True)\\n\\n\"\n        \"models_dict = {'VGG16': vgg16, 'ResNet50': resnet50, 'MobileNetV2': mobilenetv2}\"\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Task 3: Measure Inference Latency\n\nRun **100 inference passes** on a dummy image for each model. Average them to get stable latency numbers.\n\n> \ud83d\udca1 **Why 100 passes?** The first few calls are slower (GPU/memory warmup). Averaging over 100 gives a stable, representative number."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Create a dummy input image \u2014 same shape that all 3 models expect\ndummy_input = np.random.rand(1, 224, 224, 3).astype(np.float32)\n\nN_RUNS = 100\nresults = {}\n\nfor name, model in models_dict.items():\n    if model is None:\n        print(f\"\u26a0\ufe0f  {name} not loaded \u2014 skipping\")\n        continue\n\n    # TODO: Warm up the model (run once before timing)\n    # model.predict(dummy_input, verbose=0)\n\n    # TODO: Time N_RUNS inference passes\n    # start = time.time()\n    # for _ in range(N_RUNS):\n    #     model.predict(dummy_input, verbose=0)\n    # elapsed = time.time() - start\n\n    elapsed = 0  # replace with real timing\n\n    avg_ms  = (elapsed / N_RUNS) * 1000\n    fps     = 1000 / avg_ms if avg_ms > 0 else 0\n    params  = model.count_params()\n\n    results[name] = {\n        'Parameters (M)':  round(params / 1e6, 1),\n        'Latency (ms)':    round(avg_ms, 1),\n        'FPS':             round(fps, 1),\n    }\n    print(f\"{name}: {avg_ms:.1f} ms/inference  ({fps:.1f} FPS)  |  {params/1e6:.1f}M params\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "reveal_button(\n    hint_text=\"Call <code>model.predict(dummy_input, verbose=0)</code> once before the loop (warmup). \"\n              \"Then use <code>time.time()</code> before and after the loop to measure total elapsed time. \"\n              \"Average: <code>elapsed / N_RUNS * 1000</code> gives ms per inference.\",\n    solution_code=(\n        \"for name, model in models_dict.items():\\n\"\n        \"    model.predict(dummy_input, verbose=0)  # warmup\\n\"\n        \"    start = time.time()\\n\"\n        \"    for _ in range(N_RUNS):\\n\"\n        \"        model.predict(dummy_input, verbose=0)\\n\"\n        \"    elapsed = time.time() - start\\n\"\n        \"    avg_ms = (elapsed / N_RUNS) * 1000\\n\"\n        \"    fps    = 1000 / avg_ms\\n\"\n        \"    params = model.count_params()\\n\"\n        \"    results[name] = {\\n\"\n        \"        'Parameters (M)': round(params/1e6, 1),\\n\"\n        \"        'Latency (ms)':   round(avg_ms, 1),\\n\"\n        \"        'FPS':            round(fps, 1),\\n\"\n        \"    }\"\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Task 4: Build & Display the Comparison Table"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# TODO: Create a pandas DataFrame from the results dict and display it\n\n# df = pd.DataFrame(results).T\n# df.index.name = 'Model'\n\n# Style the table to highlight the best value in each column\n# df.style.highlight_min(axis=0, color='lightgreen')  # lower = better for params & latency\n\ndf = None  # replace with real DataFrame\n\nif df is not None:\n    display(df)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "reveal_button(\n    hint_text=\"<code>pd.DataFrame(results).T</code> transposes the dict into rows=models, cols=metrics. \"\n              \"Then <code>.style.highlight_min(axis=0, color='lightgreen')</code> highlights the best per column.\",\n    solution_code=(\n        \"df = pd.DataFrame(results).T\\n\"\n        \"df.index.name = 'Model'\\n\"\n        \"display(df.style.highlight_min(axis=0, color='lightgreen')\\n\"\n        \"           .highlight_max(axis=0, color='#ffcccc')\\n\"\n        \"           .format(precision=1))\"\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \ud83c\udf9a\ufe0f Task 5: Interactive Benchmark Explorer\n\nUse the controls to explore different views of the benchmark data. Which model is best depends on what you optimise for \u2014 use this to build your intuition."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "x_axis = widgets.Dropdown(\n    options=[('Parameters (M)', 'Parameters (M)'), ('Latency (ms)', 'Latency (ms)'), ('FPS', 'FPS')],\n    value='Parameters (M)', description='X axis:', layout=widgets.Layout(width='250px')\n)\ny_axis = widgets.Dropdown(\n    options=[('Latency (ms)', 'Latency (ms)'), ('FPS', 'FPS'), ('Parameters (M)', 'Parameters (M)')],\n    value='Latency (ms)', description='Y axis:', layout=widgets.Layout(width='250px')\n)\nchart_type = widgets.ToggleButtons(\n    options=['Scatter', 'Bar'], description='Chart:', button_style='info'\n)\nout_chart = widgets.Output()\n\nCOLORS = {'VGG16': '#e74c3c', 'ResNet50': '#3498db', 'MobileNetV2': '#2ecc71'}\n\ndef update_chart(change=None):\n    with out_chart:\n        out_chart.clear_output(wait=True)\n        if not results:\n            print(\"\u26a0\ufe0f  Run Tasks 2 and 3 first to populate results.\")\n            return\n\n        fig, ax = plt.subplots(figsize=(9, 5))\n\n        if chart_type.value == 'Scatter':\n            for model_name, vals in results.items():\n                ax.scatter(vals[x_axis.value], vals[y_axis.value],\n                           s=200, color=COLORS[model_name], label=model_name, zorder=5)\n                ax.annotate(f\"  {model_name}\",\n                            (vals[x_axis.value], vals[y_axis.value]),\n                            fontsize=11, fontweight='bold', color=COLORS[model_name])\n            ax.set_xlabel(x_axis.value, fontsize=12)\n            ax.set_ylabel(y_axis.value, fontsize=12)\n        else:\n            model_names = list(results.keys())\n            vals = [results[m][y_axis.value] for m in model_names]\n            bars = ax.bar(model_names, vals, color=[COLORS[m] for m in model_names], width=0.5)\n            for bar, val in zip(bars, vals):\n                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(vals)*0.01,\n                        f'{val}', ha='center', fontsize=11, fontweight='bold')\n            ax.set_ylabel(y_axis.value, fontsize=12)\n\n        ax.set_title(f'Model Comparison: {y_axis.value}', fontsize=13, fontweight='bold')\n        ax.grid(True, alpha=0.3, axis='y')\n        if chart_type.value == 'Scatter': ax.legend(fontsize=11)\n        plt.tight_layout()\n        plt.show()\n\nx_axis.observe(update_chart, names='value')\ny_axis.observe(update_chart, names='value')\nchart_type.observe(update_chart, names='value')\n\ndisplay(widgets.VBox([\n    widgets.HBox([x_axis, y_axis, chart_type]),\n    out_chart\n]))\nupdate_chart()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \u270d\ufe0f Task 6: Your Recommendation\n\nBased on the benchmark data, write **3 sentences** recommending which model to deploy on the Raspberry Pi. Your answer should justify:\n- Why you chose that model (cite specific numbers)\n- What you're sacrificing (every choice has a trade-off)\n- One condition under which you'd choose a different model instead"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# \u270f\ufe0f Your recommendation:\n# I recommend ___ because...\n# The trade-off is...\n# I would switch to ___ if..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \ud83e\udd14 Compare Against Your Predictions\n\nGo back to your predictions at the top. How close were you?\n\n| Model | Predicted params | Actual params | Predicted latency | Actual latency |\n|---|---|---|---|---|\n| VGG16 | ? | | ? | |\n| ResNet50 | ? | | ? | |\n| MobileNetV2 | ? | | ? | |\n\n**Which result surprised you the most? Why?**"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# \u270f\ufe0f What surprised me most:\n# I was most wrong about...\n# This makes sense because..."
  }
 ]
}