{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåø Lab 2.2: Transfer Learning with ResNet50\n",
    "**Module 3: Computer Vision and Image Processing**\n",
    "B-Tech AI Specialization | Chitkara University | February 2026\n",
    "\n",
    "---\n",
    "\n",
    "## üåæ Industry Scenario\n",
    "> You have **500 images** of 5 types of plant diseases. A farmer app needs a classifier to identify diseases from phone photos. Training from scratch would take days and thousands of images. **Transfer learning** lets you adapt a model that already understands images to your specific task ‚Äî quickly.\n",
    "\n",
    "## üéØ Objective\n",
    "Fine-tune a pre-trained ResNet50 on a small plant disease dataset. Compare against training from scratch. Target: **‚â•80% validation accuracy in 10 epochs**.\n",
    "\n",
    "**Time:** 120 minutes | **Mode:** Individual\n",
    "\n",
    "---\n",
    "### üìã Lab Flow\n",
    "| Stage | What happens |\n",
    "|---|---|\n",
    "| ü§î Predict | Answer before coding ‚Äî commit to a guess |\n",
    "| üíª Code | Fill in the `TODO` sections |\n",
    "| üí° Reveal | Click to check hint or full solution |\n",
    "| üéöÔ∏è Explore | Interactive plots ‚Äî dig into your results |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup ‚Äî Run First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reveal_button() ready ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "def reveal_button(hint_text, solution_code):\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, Code\n",
    "    out = widgets.Output()\n",
    "    hint_btn = widgets.Button(description='üí° Hint', button_style='info',\n",
    "        layout=widgets.Layout(width='120px', margin='4px'))\n",
    "    sol_btn  = widgets.Button(description='‚úÖ Solution', button_style='warning',\n",
    "        layout=widgets.Layout(width='140px', margin='4px'))\n",
    "    hide_btn = widgets.Button(description='üôà Hide', button_style='',\n",
    "        layout=widgets.Layout(width='100px', margin='4px'))\n",
    "    def on_hint(b):\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            display(HTML(f'<div style=\"background:#e3f2fd;padding:12px;border-radius:6px;'\n",
    "                f'border-left:4px solid #1976D2;font-size:14px\"><b>üí° Hint:</b><br>{hint_text}</div>'))\n",
    "    def on_sol(b):\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            display(HTML('<b>‚úÖ Solution:</b>'))\n",
    "            display(Code(solution_code, language='python'))\n",
    "    def on_hide(b):\n",
    "        with out: out.clear_output()\n",
    "    hint_btn.on_click(on_hint); sol_btn.on_click(on_sol); hide_btn.on_click(on_hide)\n",
    "    display(widgets.HBox([hint_btn, sol_btn, hide_btn]), out)\n",
    "\n",
    "print(\"reveal_button() ready ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è Your predictions (as comments):\n",
    "# 1. Train: 80   Validation: 20\n",
    "# 2. Validation is needed because it helps us to evaluate the model's performance on unseen data and prevents overfitting.\n",
    "# 3. With only 100 images per class, the risk is that the model may not generalize well to new data, leading to overfitting. Data augmentation can help mitigate this by artificially increasing the diversity of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/val split...\n",
      "Split created ‚úÖ\n",
      "\n",
      "Classes found (1): ['flower_photos']\n",
      "  flower_photos: 0 train | 0 val\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, zipfile, shutil, random\n",
    "\n",
    "# Download the flower_photos dataset\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "\n",
    "# get_file returns the cache dir; the tgz extracts into a nested flower_photos/ subfolder.\n",
    "# Adjust if needed so data_dir points at the folder containing the class subdirectories.\n",
    "nested = os.path.join(data_dir, 'flower_photos')\n",
    "if os.path.exists(nested):\n",
    "    data_dir = nested\n",
    "\n",
    "print(f\"data_dir: {data_dir}\")\n",
    "print(f\"Contents: {sorted(os.listdir(data_dir))}\")\n",
    "\n",
    "# flower_photos has class folders directly (daisy/, roses/, etc.) ‚Äî no train/val split.\n",
    "# We create one here: 80% train, 20% val.\n",
    "SPLIT_DIR = os.path.join(os.path.dirname(data_dir), 'flower_photos_split')\n",
    "TRAIN_DIR = os.path.join(SPLIT_DIR, 'train')\n",
    "VAL_DIR   = os.path.join(SPLIT_DIR, 'val')\n",
    "\n",
    "if not os.path.exists(SPLIT_DIR):\n",
    "    print(\"Creating train/val split...\")\n",
    "    random.seed(42)\n",
    "    class_names = [d for d in sorted(os.listdir(data_dir))\n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    for cls in class_names:\n",
    "        src_cls = os.path.join(data_dir, cls)\n",
    "        images  = [f for f in os.listdir(src_cls)\n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        random.shuffle(images)\n",
    "        split   = int(0.8 * len(images))\n",
    "        for split_name, subset in [('train', images[:split]), ('val', images[split:])]:\n",
    "            dst = os.path.join(SPLIT_DIR, split_name, cls)\n",
    "            os.makedirs(dst, exist_ok=True)\n",
    "            for img in subset:\n",
    "                shutil.copy(os.path.join(src_cls, img), os.path.join(dst, img))\n",
    "    print(\"Split created ‚úÖ\")\n",
    "else:\n",
    "    print(\"Split already exists ‚úÖ\")\n",
    "\n",
    "# Quick check ‚Äî print class names and image counts\n",
    "classes = sorted(os.listdir(TRAIN_DIR))\n",
    "print(f\"\\nClasses found ({len(classes)}): {classes}\")\n",
    "for cls in classes:\n",
    "    n_train = len(os.listdir(os.path.join(TRAIN_DIR, cls)))\n",
    "    n_val   = len(os.listdir(os.path.join(VAL_DIR,   cls)))\n",
    "    print(f\"  {cls}: {n_train} train | {n_val} val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Data Augmentation\n",
    "\n",
    "With only ~80 training images per class, we need to artificially expand the dataset using **augmentation** ‚Äî creating modified versions of each image on the fly during training.\n",
    "\n",
    "### ü§î Predict First\n",
    "Look at the augmentation parameters below. For each one, predict:\n",
    "- What does it do visually to the image?\n",
    "- Does it make sense for plant disease photos? (Would a real phone photo look like this?)\n",
    "\n",
    "| Parameter | Your prediction | Makes sense? |\n",
    "|---|---|---|\n",
    "| `horizontal_flip=True` | Flips the image horizontally (left-right) | Yes, since plant photos can be taken from either side |\n",
    "| `rotation_range=20` | Rotates the image by up to 20 degrees in either direction | Yes, real photos may be taken at slight angles |\n",
    "| `zoom_range=0.2` | Zooms in or out by up to 20% | Yes, phone cameras may zoom in or out slightly |\n",
    "| `width_shift_range=0.1` | Shifts the image horizontally by up to 10% of its width | Yes, real photos may be slightly off-center due to camera positioning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª Build the Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal_button(\n",
    "    hint_text=\"Validation generator should have <b>no augmentation</b> ‚Äî only <code>preprocessing_function</code>. \"\n",
    "              \"Augmenting validation data would give you unrealistic accuracy scores.\",\n",
    "    solution_code=(\n",
    "        \"train_datagen = ImageDataGenerator(\\n\"\n",
    "        \"    preprocessing_function=preprocess_input,\\n\"\n",
    "        \"    horizontal_flip=True,\\n\"\n",
    "        \"    rotation_range=20,\\n\"\n",
    "        \"    zoom_range=0.2,\\n\"\n",
    "        \"    width_shift_range=0.1,\\n\"\n",
    "        \"    height_shift_range=0.1\\n\"\n",
    "        \")\\n\\n\"\n",
    "        \"val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\\n\\n\"\n",
    "        \"train_generator = train_datagen.flow_from_directory(\\n\"\n",
    "        \"    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'\\n\"\n",
    "        \")\\n\"\n",
    "        \"val_generator = val_datagen.flow_from_directory(\\n\"\n",
    "        \"    VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'\\n\"\n",
    "        \")\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise augmentation ‚Äî see what the model actually trains on\n",
    "sample_batch, _ = next(train_generator)\n",
    "sample_img_raw  = sample_batch[0]\n",
    "\n",
    "# Un-preprocess for display (ResNet50 uses mean subtraction, not [0,1] scaling)\n",
    "def unpreprocess(img):\n",
    "    img = img.copy()\n",
    "    img[..., 0] += 103.939\n",
    "    img[..., 1] += 116.779\n",
    "    img[..., 2] += 123.68\n",
    "    return np.clip(img[..., ::-1] / 255.0, 0, 1)  # BGR ‚Üí RGB\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"8 Augmented Versions of the Same Image\\n\"\n",
    "             \"(What the model sees during training)\", fontsize=13, fontweight='bold')\n",
    "\n",
    "aug_gen = train_datagen.flow(\n",
    "    np.expand_dims(sample_batch[0], 0), batch_size=1\n",
    ")\n",
    "for ax in axes.flat:\n",
    "    aug_img = next(aug_gen)[0]\n",
    "    ax.imshow(unpreprocess(aug_img))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"‚úèÔ∏è Observation: How different do these look from each other?\")\n",
    "print(\"   Would you expect a plant photo from a phone to look like these?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è Your predictions:\n",
    "# 1. We freeze because...\n",
    "# 2. include_top=False means...\n",
    "# 3. GlobalAveragePooling vs Flatten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5   # adjust if your dataset has a different number\n",
    "\n",
    "# TODO: Load ResNet50 base ‚Äî no top, pretrained on ImageNet\n",
    "base_model = ResNet50(\n",
    "    # weights=...,\n",
    "    # include_top=...,\n",
    "    # input_shape=...\n",
    ")\n",
    "\n",
    "# TODO: Freeze all base model layers so they don't update during Phase 1\n",
    "# base_model.trainable = ...\n",
    "\n",
    "# TODO: Build the full model by adding a classification head\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    # layers.GlobalAveragePooling2D(),\n",
    "    # layers.Dense(256, activation='relu'),\n",
    "    # layers.Dropout(0.5),\n",
    "    # layers.Dense(NUM_CLASSES, activation='softmax'),\n",
    "])\n",
    "\n",
    "# TODO: Compile with adam and categorical_crossentropy\n",
    "# model.compile(...)\n",
    "\n",
    "# Check: how many layers are trainable?\n",
    "trainable   = sum(1 for l in model.layers[0].layers if l.trainable)\n",
    "untrainable = sum(1 for l in model.layers[0].layers if not l.trainable)\n",
    "print(f\"ResNet50 layers ‚Äî Trainable: {trainable} | Frozen: {untrainable}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Phase 1 ‚Äî Train the Classification Head (10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal_button(\n",
    "    hint_text=\"Call <code>model.fit(train_generator, epochs=10, validation_data=val_generator)</code>. \"\n",
    "              \"Store the result in <code>history_phase1</code>.\",\n",
    "    solution_code=(\n",
    "        \"history_phase1 = model.fit(\\n\"\n",
    "        \"    train_generator,\\n\"\n",
    "        \"    epochs=10,\\n\"\n",
    "        \"    validation_data=val_generator\\n\"\n",
    "        \")\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Phase 2 ‚Äî Fine-Tuning (Unfreeze Last 20 Layers)\n",
    "\n",
    "Now we'll carefully unfreeze the **last 20 layers** of ResNet50 and train them at a very low learning rate. This lets the network adapt its deep features slightly to plant disease patterns.\n",
    "\n",
    "### ü§î Predict First\n",
    "1. Why must the learning rate be **much lower** in fine-tuning (1e-5 vs 1e-3)?\n",
    "2. Why do we unfreeze only the **last** layers, not the first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è Your predictions:\n",
    "# 1. Lower LR because...\n",
    "# 2. Last layers because..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Unfreeze the last 20 layers of the base model\n",
    "base_model = model.layers[0]  # get the ResNet50 sub-model\n",
    "\n",
    "# Step 1: make base model trainable overall\n",
    "# base_model.trainable = True\n",
    "\n",
    "# Step 2: freeze everything EXCEPT the last 20 layers\n",
    "# for layer in base_model.layers[:-20]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# TODO: Re-compile with a much lower learning rate\n",
    "# model.compile(\n",
    "#     optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# Check how many are now trainable\n",
    "trainable = sum(1 for l in base_model.layers if l.trainable)\n",
    "print(f\"Now trainable layers in ResNet50: {trainable}\")\n",
    "\n",
    "# TODO: Train for 5 more epochs\n",
    "# history_phase2 = model.fit(...)\n",
    "\n",
    "history_phase2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal_button(\n",
    "    hint_text=\"<code>base_model.trainable = True</code> first, then loop: \"\n",
    "              \"<code>for layer in base_model.layers[:-20]: layer.trainable = False</code>. \"\n",
    "              \"Re-compile with <code>learning_rate=1e-5</code>.\",\n",
    "    solution_code=(\n",
    "        \"base_model = model.layers[0]\\n\"\n",
    "        \"base_model.trainable = True\\n\"\n",
    "        \"for layer in base_model.layers[:-20]:\\n\"\n",
    "        \"    layer.trainable = False\\n\\n\"\n",
    "        \"model.compile(\\n\"\n",
    "        \"    optimizer=optimizers.Adam(learning_rate=1e-5),\\n\"\n",
    "        \"    loss='categorical_crossentropy',\\n\"\n",
    "        \"    metrics=['accuracy']\\n\"\n",
    "        \")\\n\\n\"\n",
    "        \"history_phase2 = model.fit(\\n\"\n",
    "        \"    train_generator, epochs=5, validation_data=val_generator\\n\"\n",
    "        \")\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéöÔ∏è Task 6: Explore ‚Äî Interactive Training Curves\n",
    "\n",
    "Use the controls below to examine your training history. Look for:\n",
    "- Where does Phase 1 plateau? Where does Phase 2 give an extra push?\n",
    "- Is there a gap between train and val accuracy? What does that mean?\n",
    "- At what epoch does the model first exceed 80% validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined history from both phases\n",
    "def build_history_dict(h1, h2):\n",
    "    \"\"\"Merge two History objects into one dict for plotting.\"\"\"\n",
    "    combined = {}\n",
    "    for key in h1.history:\n",
    "        p2_vals = h2.history.get(key, [])\n",
    "        combined[key] = h1.history[key] + p2_vals\n",
    "    combined['phase_boundary'] = len(h1.history['accuracy'])\n",
    "    return combined\n",
    "\n",
    "# ‚îÄ‚îÄ Interactive curve explorer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "metric_toggle = widgets.ToggleButtons(\n",
    "    options=[('Accuracy', 'accuracy'), ('Loss', 'loss')],\n",
    "    description='Metric:', button_style='info'\n",
    ")\n",
    "show_phases = widgets.Checkbox(value=True, description='Show phase boundary')\n",
    "smooth_check = widgets.Checkbox(value=False, description='Smooth curves')\n",
    "out_plot = widgets.Output()\n",
    "\n",
    "def update_curves(change=None):\n",
    "    if history_phase1 is None or history_phase2 is None:\n",
    "        with out_plot:\n",
    "            out_plot.clear_output()\n",
    "            print(\"‚ö†Ô∏è  Run Tasks 4 and 5 first to generate training history.\")\n",
    "        return\n",
    "\n",
    "    hist = build_history_dict(history_phase1, history_phase2)\n",
    "    metric   = metric_toggle.value\n",
    "    val_key  = f'val_{metric}'\n",
    "    boundary = hist['phase_boundary']\n",
    "    epochs   = list(range(1, len(hist[metric]) + 1))\n",
    "\n",
    "    def smooth(vals, w=3):\n",
    "        return [np.mean(vals[max(0,i-w):i+1]) for i in range(len(vals))]\n",
    "\n",
    "    train_vals = smooth(hist[metric])     if smooth_check.value else hist[metric]\n",
    "    val_vals   = smooth(hist[val_key])    if smooth_check.value else hist[val_key]\n",
    "\n",
    "    with out_plot:\n",
    "        out_plot.clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        ax.plot(epochs, train_vals, 'b-o', markersize=5, label=f'Train {metric}', linewidth=2)\n",
    "        ax.plot(epochs, val_vals,   'r-o', markersize=5, label=f'Val {metric}',   linewidth=2)\n",
    "\n",
    "        if show_phases.value:\n",
    "            ax.axvline(x=boundary + 0.5, color='purple', linestyle='--', alpha=0.7, linewidth=2)\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            ax.text(boundary * 0.5, ymax * 0.97, 'Phase 1\\n(frozen)', ha='center',\n",
    "                    color='purple', fontsize=10, fontweight='bold')\n",
    "            ax.text(boundary + (len(epochs) - boundary) * 0.5, ymax * 0.97, 'Phase 2\\n(fine-tune)',\n",
    "                    ha='center', color='purple', fontsize=10, fontweight='bold')\n",
    "\n",
    "        if metric == 'accuracy':\n",
    "            ax.axhline(y=0.80, color='green', linestyle=':', alpha=0.8, linewidth=1.5,\n",
    "                       label='80% target')\n",
    "            best_val  = max(val_vals)\n",
    "            best_ep   = val_vals.index(best_val) + 1\n",
    "            ax.annotate(f'Best: {best_val:.1%} @ ep{best_ep}',\n",
    "                        xy=(best_ep, best_val), xytext=(best_ep + 1, best_val - 0.05),\n",
    "                        arrowprops=dict(arrowstyle='->', color='red'), color='red', fontsize=10)\n",
    "\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
    "        ax.set_title(f'Training Curves ‚Äî Transfer Learning with ResNet50', fontsize=13, fontweight='bold')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "metric_toggle.observe(update_curves, names='value')\n",
    "show_phases.observe(update_curves, names='value')\n",
    "smooth_check.observe(update_curves, names='value')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([metric_toggle, show_phases, smooth_check]),\n",
    "    out_plot\n",
    "]))\n",
    "update_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úçÔ∏è Reflection\n",
    "\n",
    "Write a short paragraph (3‚Äì5 sentences) explaining:\n",
    "- Why did transfer learning work so well with only 500 images?\n",
    "- What did Phase 1 learn vs Phase 2?\n",
    "- Would you expect the same result if you trained from scratch? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è Your reflection:\n",
    "# Transfer learning worked well because...\n",
    "# In Phase 1, the model learned...\n",
    "# In Phase 2, fine-tuning added...\n",
    "# Training from scratch would have..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
