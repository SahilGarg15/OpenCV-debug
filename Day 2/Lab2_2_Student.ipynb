{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# \ud83c\udf3f Lab 2.2: Transfer Learning with ResNet50\n**Module 3: Computer Vision and Image Processing**\nB-Tech AI Specialization | Chitkara University | February 2026\n\n---\n\n## \ud83c\udf3e Industry Scenario\n> You have **500 images** of 5 types of plant diseases. A farmer app needs a classifier to identify diseases from phone photos. Training from scratch would take days and thousands of images. **Transfer learning** lets you adapt a model that already understands images to your specific task \u2014 quickly.\n\n## \ud83c\udfaf Objective\nFine-tune a pre-trained ResNet50 on a small plant disease dataset. Compare against training from scratch. Target: **\u226580% validation accuracy in 10 epochs**.\n\n**Time:** 120 minutes | **Mode:** Individual\n\n---\n### \ud83d\udccb Lab Flow\n| Stage | What happens |\n|---|---|\n| \ud83e\udd14 Predict | Answer before coding \u2014 commit to a guess |\n| \ud83d\udcbb Code | Fill in the `TODO` sections |\n| \ud83d\udca1 Reveal | Click to check hint or full solution |\n| \ud83c\udf9a\ufe0f Explore | Interactive plots \u2014 dig into your results |\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \u2699\ufe0f Setup \u2014 Run First"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from google.colab import output\noutput.enable_custom_widget_manager()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import History\n\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, Code\nimport os, time, warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"TensorFlow : {tf.__version__}\")\nprint(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\nprint(\"\u2705 Ready\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def reveal_button(hint_text, solution_code):\n    import ipywidgets as widgets\n    from IPython.display import display, HTML, Code\n    out = widgets.Output()\n    hint_btn = widgets.Button(description='\ud83d\udca1 Hint', button_style='info',\n        layout=widgets.Layout(width='120px', margin='4px'))\n    sol_btn  = widgets.Button(description='\u2705 Solution', button_style='warning',\n        layout=widgets.Layout(width='140px', margin='4px'))\n    hide_btn = widgets.Button(description='\ud83d\ude48 Hide', button_style='',\n        layout=widgets.Layout(width='100px', margin='4px'))\n    def on_hint(b):\n        with out:\n            out.clear_output(wait=True)\n            display(HTML(f'<div style=\"background:#e3f2fd;padding:12px;border-radius:6px;'\n                f'border-left:4px solid #1976D2;font-size:14px\"><b>\ud83d\udca1 Hint:</b><br>{hint_text}</div>'))\n    def on_sol(b):\n        with out:\n            out.clear_output(wait=True)\n            display(HTML('<b>\u2705 Solution:</b>'))\n            display(Code(solution_code, language='python'))\n    def on_hide(b):\n        with out: out.clear_output()\n    hint_btn.on_click(on_hint); sol_btn.on_click(on_sol); hide_btn.on_click(on_hide)\n    display(widgets.HBox([hint_btn, sol_btn, hide_btn]), out)\n\nprint(\"reveal_button() ready \u2705\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Task 1: Prepare the Dataset\n\nWe'll use a small subset of the **PlantVillage** dataset \u2014 5 plant disease classes, 100 images each.\n\n### \ud83e\udd14 Predict First\nBefore running any code, answer these:\n1. We have 500 images total. How many will be in train vs. validation (80/20 split)?\n2. Why do we need validation data at all \u2014 why not just train on everything?\n3. What problems could arise with only 100 images per class?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# \u270f\ufe0f Your predictions (as comments):\n# 1. Train: ___   Validation: ___\n# 2. Validation is needed because...\n# 3. With only 100 images per class, the risk is..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### \ud83d\udcbb Download and Organise the Dataset\nWe'll download a pre-prepared subset from a public source and set up directory structure."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Download dataset \u2014 this creates a folder structure:\n# data/\n#   train/\n#     Tomato_Bacterial_spot/  (80 images)\n#     Tomato_Early_blight/    (80 images)\n#     ... (5 classes total)\n#   val/\n#     Tomato_Bacterial_spot/  (20 images)\n#     ...\n\nimport urllib.request, zipfile\n\nDATA_URL  = \"https://github.com/btphan95/greenr-dataset/raw/master/data.zip\"\nDATA_DIR  = \"plant_disease_data\"\n\n# TODO: Download and extract the dataset\n# Uncomment and complete:\n# if not os.path.exists(DATA_DIR):\n#     print(\"Downloading dataset...\")\n#     urllib.request.urlretrieve(DATA_URL, \"data.zip\")\n#     with zipfile.ZipFile(\"data.zip\", 'r') as z:\n#         z.extractall(DATA_DIR)\n#     print(\"Extracted \u2705\")\n\n# --- ALTERNATIVE: If the URL above doesn't work, use tf.keras.utils.get_file ---\n# dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n# data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n\n# TODO: Set your train and validation directory paths\nTRAIN_DIR = None   # e.g. os.path.join(DATA_DIR, 'train')\nVAL_DIR   = None   # e.g. os.path.join(DATA_DIR, 'val')\n\n# Quick check \u2014 print class names and image counts\nif TRAIN_DIR and os.path.exists(TRAIN_DIR):\n    classes = sorted(os.listdir(TRAIN_DIR))\n    print(f\"Classes found ({len(classes)}): {classes}\")\n    for cls in classes:\n        n = len(os.listdir(os.path.join(TRAIN_DIR, cls)))\n        print(f\"  {cls}: {n} training images\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "reveal_button(\n    hint_text=\"Use <code>urllib.request.urlretrieve(url, filename)</code> to download, \"\n              \"then <code>zipfile.ZipFile</code> to extract. Set TRAIN_DIR and VAL_DIR \"\n              \"to point at the <code>train/</code> and <code>val/</code> subdirectories.\",\n    solution_code=(\n        \"if not os.path.exists(DATA_DIR):\\n\"\n        \"    urllib.request.urlretrieve(DATA_URL, 'data.zip')\\n\"\n        \"    with zipfile.ZipFile('data.zip', 'r') as z:\\n\"\n        \"        z.extractall(DATA_DIR)\\n\\n\"\n        \"TRAIN_DIR = os.path.join(DATA_DIR, 'train')\\n\"\n        \"VAL_DIR   = os.path.join(DATA_DIR, 'val')\"\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Task 2: Data Augmentation\n\nWith only ~80 training images per class, we need to artificially expand the dataset using **augmentation** \u2014 creating modified versions of each image on the fly during training.\n\n### \ud83e\udd14 Predict First\nLook at the augmentation parameters below. For each one, predict:\n- What does it do visually to the image?\n- Does it make sense for plant disease photos? (Would a real phone photo look like this?)\n\n| Parameter | Your prediction | Makes sense? |\n|---|---|---|\n| `horizontal_flip=True` | ? | ? |\n| `rotation_range=20` | ? | ? |\n| `zoom_range=0.2` | ? | ? |\n| `width_shift_range=0.1` | ? | ? |"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# \u270f\ufe0f Fill in your predictions in the table above (edit the markdown cell)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### \ud83d\udcbb Build the Data Generators"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "IMG_SIZE  = (224, 224)\nBATCH_SIZE = 32\n\n# TODO: Create an ImageDataGenerator for TRAINING with augmentation\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    # horizontal_flip=...,\n    # rotation_range=...,\n    # zoom_range=...,\n    # width_shift_range=...,\n    # height_shift_range=...,\n)\n\n# TODO: Create a separate generator for VALIDATION \u2014 no augmentation, just preprocessing\nval_datagen = ImageDataGenerator(\n    # preprocessing_function=...\n)\n\n# TODO: Create the flow_from_directory generators\ntrain_generator = None\n# train_generator = train_datagen.flow_from_directory(\n#     TRAIN_DIR,\n#     target_size=IMG_SIZE,\n#     batch_size=BATCH_SIZE,\n#     class_mode='categorical'\n# )\n\nval_generator = None\n# val_generator = val_datagen.flow_from_directory(...)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "reveal_button(\n    hint_text=\"Validation generator should have <b>no augmentation</b> \u2014 only <code>preprocessing_function</code>. \"\n              \"Augmenting validation data would give you unrealistic accuracy scores.\",\n    solution_code=(\n        \"train_datagen = ImageDataGenerator(\\n\"\n        \"    preprocessing_function=preprocess_input,\\n\"\n        \"    horizontal_flip=True,\\n\"\n        \"    rotation_range=20,\\n\"\n        \"    zoom_range=0.2,\\n\"\n        \"    width_shift_range=0.1,\\n\"\n        \"    height_shift_range=0.1\\n\"\n        \")\\n\\n\"\n        \"val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\\n\\n\"\n        \"train_generator = train_datagen.flow_from_directory(\\n\"\n        \"    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'\\n\"\n        \")\\n\"\n        \"val_generator = val_datagen.flow_from_directory(\\n\"\n        \"    VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical'\\n\"\n        \")\"\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### \ud83c\udf9a\ufe0f Explore: What Does Augmentation Actually Do?\nRun the cell below to visualise 8 augmented versions of the same image side-by-side."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Visualise augmentation \u2014 see what the model actually trains on\nsample_batch, _ = next(train_generator)\nsample_img_raw  = sample_batch[0]\n\n# Un-preprocess for display (ResNet50 uses mean subtraction, not [0,1] scaling)\ndef unpreprocess(img):\n    img = img.copy()\n    img[..., 0] += 103.939\n    img[..., 1] += 116.779\n    img[..., 2] += 123.68\n    return np.clip(img[..., ::-1] / 255.0, 0, 1)  # BGR \u2192 RGB\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\nfig.suptitle(\"8 Augmented Versions of the Same Image\\n\"\n             \"(What the model sees during training)\", fontsize=13, fontweight='bold')\n\naug_gen = train_datagen.flow(\n    np.expand_dims(sample_batch[0], 0), batch_size=1\n)\nfor ax in axes.flat:\n    aug_img = next(aug_gen)[0]\n    ax.imshow(unpreprocess(aug_img))\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\nprint(\"\u270f\ufe0f Observation: How different do these look from each other?\")\nprint(\"   Would you expect a plant photo from a phone to look like these?\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Task 3: Build the Transfer Learning Model (Feature Extraction Phase)\n\nTransfer learning has two phases:\n\n```\nPhase 1 \u2014 Feature Extraction:\n  ResNet50 (frozen, pretrained) \u2192 GlobalAveragePooling \u2192 Dense \u2192 Softmax\n  \u2191 weights locked, won't change              \u2191 only these train\n\nPhase 2 \u2014 Fine-tuning:\n  ResNet50 (last 20 layers UNfrozen) \u2192 GlobalAveragePooling \u2192 Dense \u2192 Softmax\n  \u2191 these now also update, but slowly\n```\n\n### \ud83e\udd14 Predict First\n1. Why do we **freeze** ResNet50's layers in Phase 1?\n2. Why do we need `include_top=False`?\n3. What does `GlobalAveragePooling2D` do differently from `Flatten`?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# \u270f\ufe0f Your predictions:\n# 1. We freeze because...\n# 2. include_top=False means...\n# 3. GlobalAveragePooling vs Flatten:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### \ud83d\udcbb Build the Model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "NUM_CLASSES = 5   # adjust if your dataset has a different number\n\n# TODO: Load ResNet50 base \u2014 no top, pretrained on ImageNet\nbase_model = ResNet50(\n    # weights=...,\n    # include_top=...,\n    # input_shape=...\n)\n\n# TODO: Freeze all base model layers so they don't update during Phase 1\n# base_model.trainable = ...\n\n# TODO: Build the full model by adding a classification head\nmodel = models.Sequential([\n    base_model,\n    # layers.GlobalAveragePooling2D(),\n    # layers.Dense(256, activation='relu'),\n    # layers.Dropout(0.5),\n    # layers.Dense(NUM_CLASSES, activation='softmax'),\n])\n\n# TODO: Compile with adam and categorical_crossentropy\n# model.compile(...)\n\n# Check: how many layers are trainable?\ntrainable   = sum(1 for l in model.layers[0].layers if l.trainable)\nuntrainable = sum(1 for l in model.layers[0].layers if not l.trainable)\nprint(f\"ResNet50 layers \u2014 Trainable: {trainable} | Frozen: {untrainable}\")\nmodel.summary()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "reveal_button(\n    hint_text=\"Set <code>base_model.trainable = False</code> after loading. \"\n              \"Then stack: <code>GlobalAveragePooling2D \u2192 Dense(256, relu) \u2192 Dropout(0.5) \u2192 Dense(NUM_CLASSES, softmax)</code>.\",\n    solution_code=(\n        \"base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\\n\"\n        \"base_model.trainable = False\\n\\n\"\n        \"model = models.Sequential([\\n\"\n        \"    base_model,\\n\"\n        \"    layers.GlobalAveragePooling2D(),\\n\"\n        \"    layers.Dense(256, activation='relu'),\\n\"\n        \"    layers.Dropout(0.5),\\n\"\n        \"    layers.Dense(NUM_CLASSES, activation='softmax'),\\n\"\n        \"])\\n\\n\"\n        \"model.compile(\\n\"\n        \"    optimizer=optimizers.Adam(learning_rate=1e-3),\\n\"\n        \"    loss='categorical_crossentropy',\\n\"\n        \"    metrics=['accuracy']\\n\"\n        \")\"\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Task 4: Phase 1 \u2014 Train the Classification Head (10 epochs)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# TODO: Train the model for 10 epochs\n# history_phase1 = model.fit(\n#     train_generator,\n#     epochs=10,\n#     validation_data=val_generator\n# )\n\nhistory_phase1 = None  # replace with model.fit(...)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "reveal_button(\n    hint_text=\"Call <code>model.fit(train_generator, epochs=10, validation_data=val_generator)</code>. \"\n              \"Store the result in <code>history_phase1</code>.\",\n    solution_code=(\n        \"history_phase1 = model.fit(\\n\"\n        \"    train_generator,\\n\"\n        \"    epochs=10,\\n\"\n        \"    validation_data=val_generator\\n\"\n        \")\"\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Task 5: Phase 2 \u2014 Fine-Tuning (Unfreeze Last 20 Layers)\n\nNow we'll carefully unfreeze the **last 20 layers** of ResNet50 and train them at a very low learning rate. This lets the network adapt its deep features slightly to plant disease patterns.\n\n### \ud83e\udd14 Predict First\n1. Why must the learning rate be **much lower** in fine-tuning (1e-5 vs 1e-3)?\n2. Why do we unfreeze only the **last** layers, not the first?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# \u270f\ufe0f Your predictions:\n# 1. Lower LR because...\n# 2. Last layers because..."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# TODO: Unfreeze the last 20 layers of the base model\nbase_model = model.layers[0]  # get the ResNet50 sub-model\n\n# Step 1: make base model trainable overall\n# base_model.trainable = True\n\n# Step 2: freeze everything EXCEPT the last 20 layers\n# for layer in base_model.layers[:-20]:\n#     layer.trainable = False\n\n# TODO: Re-compile with a much lower learning rate\n# model.compile(\n#     optimizer=optimizers.Adam(learning_rate=1e-5),\n#     loss='categorical_crossentropy',\n#     metrics=['accuracy']\n# )\n\n# Check how many are now trainable\ntrainable = sum(1 for l in base_model.layers if l.trainable)\nprint(f\"Now trainable layers in ResNet50: {trainable}\")\n\n# TODO: Train for 5 more epochs\n# history_phase2 = model.fit(...)\n\nhistory_phase2 = None"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "reveal_button(\n    hint_text=\"<code>base_model.trainable = True</code> first, then loop: \"\n              \"<code>for layer in base_model.layers[:-20]: layer.trainable = False</code>. \"\n              \"Re-compile with <code>learning_rate=1e-5</code>.\",\n    solution_code=(\n        \"base_model = model.layers[0]\\n\"\n        \"base_model.trainable = True\\n\"\n        \"for layer in base_model.layers[:-20]:\\n\"\n        \"    layer.trainable = False\\n\\n\"\n        \"model.compile(\\n\"\n        \"    optimizer=optimizers.Adam(learning_rate=1e-5),\\n\"\n        \"    loss='categorical_crossentropy',\\n\"\n        \"    metrics=['accuracy']\\n\"\n        \")\\n\\n\"\n        \"history_phase2 = model.fit(\\n\"\n        \"    train_generator, epochs=5, validation_data=val_generator\\n\"\n        \")\"\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \ud83c\udf9a\ufe0f Task 6: Explore \u2014 Interactive Training Curves\n\nUse the controls below to examine your training history. Look for:\n- Where does Phase 1 plateau? Where does Phase 2 give an extra push?\n- Is there a gap between train and val accuracy? What does that mean?\n- At what epoch does the model first exceed 80% validation accuracy?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Build combined history from both phases\ndef build_history_dict(h1, h2):\n    \"\"\"Merge two History objects into one dict for plotting.\"\"\"\n    combined = {}\n    for key in h1.history:\n        p2_vals = h2.history.get(key, [])\n        combined[key] = h1.history[key] + p2_vals\n    combined['phase_boundary'] = len(h1.history['accuracy'])\n    return combined\n\n# \u2500\u2500 Interactive curve explorer \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nmetric_toggle = widgets.ToggleButtons(\n    options=[('Accuracy', 'accuracy'), ('Loss', 'loss')],\n    description='Metric:', button_style='info'\n)\nshow_phases = widgets.Checkbox(value=True, description='Show phase boundary')\nsmooth_check = widgets.Checkbox(value=False, description='Smooth curves')\nout_plot = widgets.Output()\n\ndef update_curves(change=None):\n    if history_phase1 is None or history_phase2 is None:\n        with out_plot:\n            out_plot.clear_output()\n            print(\"\u26a0\ufe0f  Run Tasks 4 and 5 first to generate training history.\")\n        return\n\n    hist = build_history_dict(history_phase1, history_phase2)\n    metric   = metric_toggle.value\n    val_key  = f'val_{metric}'\n    boundary = hist['phase_boundary']\n    epochs   = list(range(1, len(hist[metric]) + 1))\n\n    def smooth(vals, w=3):\n        return [np.mean(vals[max(0,i-w):i+1]) for i in range(len(vals))]\n\n    train_vals = smooth(hist[metric])     if smooth_check.value else hist[metric]\n    val_vals   = smooth(hist[val_key])    if smooth_check.value else hist[val_key]\n\n    with out_plot:\n        out_plot.clear_output(wait=True)\n        fig, ax = plt.subplots(figsize=(10, 5))\n\n        ax.plot(epochs, train_vals, 'b-o', markersize=5, label=f'Train {metric}', linewidth=2)\n        ax.plot(epochs, val_vals,   'r-o', markersize=5, label=f'Val {metric}',   linewidth=2)\n\n        if show_phases.value:\n            ax.axvline(x=boundary + 0.5, color='purple', linestyle='--', alpha=0.7, linewidth=2)\n            ymin, ymax = ax.get_ylim()\n            ax.text(boundary * 0.5, ymax * 0.97, 'Phase 1\\n(frozen)', ha='center',\n                    color='purple', fontsize=10, fontweight='bold')\n            ax.text(boundary + (len(epochs) - boundary) * 0.5, ymax * 0.97, 'Phase 2\\n(fine-tune)',\n                    ha='center', color='purple', fontsize=10, fontweight='bold')\n\n        if metric == 'accuracy':\n            ax.axhline(y=0.80, color='green', linestyle=':', alpha=0.8, linewidth=1.5,\n                       label='80% target')\n            best_val  = max(val_vals)\n            best_ep   = val_vals.index(best_val) + 1\n            ax.annotate(f'Best: {best_val:.1%} @ ep{best_ep}',\n                        xy=(best_ep, best_val), xytext=(best_ep + 1, best_val - 0.05),\n                        arrowprops=dict(arrowstyle='->', color='red'), color='red', fontsize=10)\n\n        ax.set_xlabel('Epoch', fontsize=12)\n        ax.set_ylabel(metric.capitalize(), fontsize=12)\n        ax.set_title(f'Training Curves \u2014 Transfer Learning with ResNet50', fontsize=13, fontweight='bold')\n        ax.legend(fontsize=11)\n        ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n\nmetric_toggle.observe(update_curves, names='value')\nshow_phases.observe(update_curves, names='value')\nsmooth_check.observe(update_curves, names='value')\n\ndisplay(widgets.VBox([\n    widgets.HBox([metric_toggle, show_phases, smooth_check]),\n    out_plot\n]))\nupdate_curves()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## \u270d\ufe0f Reflection\n\nWrite a short paragraph (3\u20135 sentences) explaining:\n- Why did transfer learning work so well with only 500 images?\n- What did Phase 1 learn vs Phase 2?\n- Would you expect the same result if you trained from scratch? Why?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# \u270f\ufe0f Your reflection:\n# Transfer learning worked well because...\n# In Phase 1, the model learned...\n# In Phase 2, fine-tuning added...\n# Training from scratch would have..."
  }
 ]
}